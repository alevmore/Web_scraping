This Scrapy project aims to scrape quotes along with their authors from the "Quotes to Scrape" website. The project is structured to extract quotes, authors' full names, birth dates, birth locations, and descriptions, and store the data in JSON format.
Key Components
1.	Items:
o	QuoteItem: Defines the structure for storing quotes, including the quote text, author, and associated tags.
o	AuthorItem: Defines the structure for storing author details, including the author's full name, birth date, birth location, and description.
2.	Data Pipeline:
o	DataPipeline collects the extracted quotes and authors in lists and writes them to quotes.json and authors.json files once the spider is closed.
o	The process_item method processes items based on their type (quote or author) and appends them to the respective lists.
o	The close_spider method saves the collected data to JSON files.
3.	Spider:
o	QuotesSpider is the main spider that performs the web scraping. It starts from the main quotes page and follows pagination to scrape multiple pages.
o	The parse method extracts quotes and navigates to author detail pages to gather additional information.
o	The parse_author method extracts author details from their individual pages.
4.	Running the Spider:
o	The spider is initiated with CrawlerProcess to start the scraping when the script is run as a standalone program.
5.	MongoDB Connection:
o	The script establishes a connection to a MongoDB server running on the local machine (localhost) on the default port (27017).
o	It accesses the quotes database, which serves as a container for the collections that hold quote and author data.
6.	Data Import from JSON:
o	The script opens a JSON files (“Authors.json”, “Quotes.json”) containing quotes and authors and loads the data into a Python dictionary using the json library.
o	It inserts the loaded data into the quotes collection using the insert_many method, which allows for batch insertion of multiple documents at once.
o	A similar process is followed for importing author data from a separate JSON file into the authors collection.

Technologies Used
•	Scrapy: A powerful and popular web scraping framework for Python that facilitates the extraction of data from websites. It provides built-in support for handling requests, parsing responses, and managing scraped data.
•	XPath: A query language for selecting nodes from an XML document, which is used here for extracting specific elements from the HTML structure of the website.
•	JSON: A lightweight data interchange format that is used to store the scraped data in a structured way, making it easy to read and parse.
•	ItemAdapter: A utility class from the itemadapter module that allows for easier manipulation of item data by adapting items to a dictionary-like interface.
•	MongoDB: A NoSQL database that stores data in flexible, JSON-like documents. It is designed for scalability and performance, making it a popular choice for applications requiring large volumes of data.
•	PyMongo: A Python driver for MongoDB that allows developers to interact with a MongoDB database from Python code. It provides methods for connecting to the database, querying data, and inserting or updating documents.
